{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf2f675-2c27-42a8-b05b-c7b50840ee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mss\n",
    "!pip install matplotlib\n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6abbb04-bd69-4f55-bb60-83ed10164b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af99ca-18a5-405f-8a6e-dfe1d0a674fc",
   "metadata": {},
   "source": [
    "### camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a2bed-5bca-4d68-9fef-414956377d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gamenv import GameEnv\n",
    "\n",
    "game_env = GameEnv(\"snes9x.exe\", \"mario - Snes9x 1.62.3\", (100, 200, -10, -20))\n",
    "camera = game_env.camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.set_foreground_game()\n",
    "frame = camera.get_frame()\n",
    "plt.imshow(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 5_000\n",
    "GAMMA = 0.9999\n",
    "LR = 0.0001\n",
    "\n",
    "NUM_EPISODES = 5000\n",
    "SEQUENCE_LENGTH = 5\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "EPS_NUM_STEPS = 1000\n",
    "EPS_START = 0.6\n",
    "EPS_END = 0.0000001\n",
    "EPS_DECAY = (EPS_START - EPS_END) / (EPS_NUM_STEPS)\n",
    "\n",
    "\n",
    "action_space = 4\n",
    "\n",
    "load_model = False\n",
    "model_name = \"2023_12_19_14_01_33\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from itertools import count\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from src.memory import Memory\n",
    "\n",
    "timestr = time.strftime(\"%Y_%m_%d_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initiliaze replay memory D to capacity N\n",
    "replay_memory = Memory(D)\n",
    "sequence = Memory(SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be6948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.batch_model import DQN, phi\n",
    "from torch import nn\n",
    "\n",
    "# Initialize action-value function Q with random weights\n",
    "\n",
    "lstm_n = 256\n",
    "lstm_layers = 2\n",
    "\n",
    "policy_net = DQN(action_space, lstm_n, lstm_layers).to(device) # used to store teta\n",
    "target_net = DQN(action_space, lstm_n, lstm_layers).to(device) # used to store teta-1\n",
    "\n",
    "if load_model:\n",
    "    policy_net.load_state_dict(torch.load(f'./saved_models/{model_name}/policy_net'))\n",
    "    \n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(policy_net.parameters(), lr=LR)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "steps = 0\n",
    "episode = 0\n",
    "epsilon = EPS_START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(2)\n",
    "camera.set_foreground_game()\n",
    "time.sleep(0.2)\n",
    "\n",
    "with SummaryWriter(logdir=\"runs/\"+timestr) as writer:\n",
    "    \n",
    "    while steps < D-50:\n",
    "        \n",
    "        ep_rewards = 0\n",
    "        ep_qvalues = 0\n",
    "        ep_loss = 0\n",
    "        \n",
    "        # Initialise sequence s1 = {x1} and preprocessed sequenced φ1 = φ(s1)\n",
    "        observation = game_env.reset()\n",
    "        state = phi(observation, device)\n",
    "        last_reward = 0\n",
    "        sequence.clear()\n",
    "        sequence.push(state)\n",
    "        \n",
    "        rendered, sequence_state = sequence.render()\n",
    "        \n",
    "        for t in count():\n",
    "\n",
    "            hn = torch.zeros(lstm_layers, 1, lstm_n, dtype=torch.float32, device=device)\n",
    "            cn = torch.zeros(lstm_layers, 1, lstm_n, dtype=torch.float32, device=device)\n",
    "            with torch.no_grad():\n",
    "                if rendered:\n",
    "                    q_value_action, hn, cn = policy_net(sequence_state.unsqueeze(0), hn, cn)\n",
    "                    q_value_action = q_value_action.max(1)\n",
    "                    q_value = q_value_action[0].item()\n",
    "                    ep_qvalues += q_value\n",
    "                else:\n",
    "                    action = torch.tensor([[random.randint(0, action_space-1)]], device=device, dtype=torch.long)\n",
    "                \n",
    "            \n",
    "            # With probability eps select a random action at\n",
    "            epsilon = epsilon - EPS_DECAY if epsilon > EPS_END else EPS_END\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = torch.tensor([[random.randint(0, action_space-1)]], device=device, dtype=torch.long)\n",
    "            # otherwise select at = maxaQ*(phi(st), a; teta)\n",
    "            else:\n",
    "                if rendered:\n",
    "                    action = q_value_action[1].view(1,1)\n",
    "                    print(action)\n",
    "\n",
    "            # Execute action at in emulator and observe reward rt and image xt+1\n",
    "            observation, reward, game_over = game_env.step(action.item())\n",
    "            \n",
    "            ep_rewards += reward\n",
    "            reward = torch.tensor([reward], device=device, dtype=torch.float32)\n",
    "\n",
    "            # Set st+1 = st, at, xt+1 and preprocess phit+1 = phi(st+1)\n",
    "            if game_over:\n",
    "                next_state = None # ordem provavelmente errada\n",
    "            else:\n",
    "                next_state = phi(observation, device)\n",
    "                sequence.push(next_state)\n",
    "                rendered, next_sequence_state = sequence.render()\n",
    "\n",
    "            if sequence_state != None:\n",
    "                #replay_memory.push((sequence_state, action, reward, next_sequence_state))\n",
    "                replay_memory.push((sequence_state, action, reward, next_state))\n",
    "            \n",
    "            if len(replay_memory) > BATCH_SIZE and steps > 50:\n",
    "                loss = policy_net.q_train(target_net, optimizer, loss_fn, replay_memory.sample(BATCH_SIZE), GAMMA, lstm_n, lstm_layers, device).item()\n",
    "                ep_loss += loss\n",
    "                if t % 150 == 0:\n",
    "\n",
    "                    clear_output(wait=True)\n",
    "                    print('loss:', ep_loss, 'step:', steps, 'epsilon:', epsilon, 'last_reward:', ep_rewards, 'ep:', episode)\n",
    "\n",
    "                    writer.add_scalar(\"Loss/ep_loss\", ep_loss, steps)\n",
    "                    writer.add_scalar(\"Reward/ep_rewards\", ep_rewards, steps)\n",
    "                    writer.add_scalar(\"Qvalue/ep_qvalues\", ep_qvalues, steps)\n",
    "\n",
    "                    ep_rewards = 0\n",
    "                    ep_qvalues = 0\n",
    "                    ep_loss = 0\n",
    "\n",
    "                writer.add_scalar(\"Loss/loss\", loss, steps)\n",
    "                writer.add_scalar(\"Reward/reward\", reward, steps)\n",
    "                writer.add_scalar(\"Qvalue/qvalue\", q_value, steps)\n",
    "                writer.flush()\n",
    "                \n",
    "                if t % 300 == 0:\n",
    "                    target_net.load_state_dict(policy_net.state_dict())\n",
    "                    \n",
    "            sequence_state = next_sequence_state \n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            if game_over:\n",
    "                break\n",
    "        episode += 1\n",
    "        # writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.set_foreground_game()\n",
    "while True:\n",
    "    \n",
    "    observation = game_env.reset()\n",
    "    time.sleep(0.1)\n",
    "    state = phi(observation, device)\n",
    "    sequence.clear()\n",
    "    sequence.push(state)\n",
    "    \n",
    "    rendered, sequence_state = sequence.render()\n",
    "    \n",
    "    for t in count():\n",
    "\n",
    "        hn = torch.zeros(lstm_layers, 1, lstm_n, dtype=torch.float32, device=device)\n",
    "        cn = torch.zeros(lstm_layers, 1, lstm_n, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            if rendered:\n",
    "                q_value_action, hn, cn = policy_net(sequence_state.unsqueeze(0), hn, cn)\n",
    "                q_value_action = q_value_action.max(1)\n",
    "                action = q_value_action[1].view(1,1)\n",
    "            else:\n",
    "                action = torch.tensor([[random.randint(0, action_space-1)]], device=device, dtype=torch.long)      \n",
    "        print(action)\n",
    "        observation, reward, game_over = game_env.step(action.item())\n",
    "        if game_over:\n",
    "            next_state = None # ordem provavelmente errada\n",
    "        else:\n",
    "            next_state = phi(observation, device)\n",
    "            sequence.push(next_state)\n",
    "            rendered, next_sequence_state = sequence.render()\n",
    "        sequence_state = next_sequence_state\n",
    "        if game_over:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "save_model_dir = './saved_models'\n",
    "\n",
    "if not os.path.exists(f'{save_model_dir}'):\n",
    "    os.mkdir(f'{save_model_dir}')\n",
    "if not os.path.exists(f'{save_model_dir}/{timestr}'):\n",
    "    os.mkdir(f'{save_model_dir}/{timestr}')\n",
    "\n",
    "torch.save(policy_net.state_dict(), f'{save_model_dir}/{timestr}/policy_net')\n",
    "torch.save(target_net.state_dict(), f'{save_model_dir}/{timestr}/target_net')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
